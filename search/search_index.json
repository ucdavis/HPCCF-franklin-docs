{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Franklin","text":"<p>Franklin is a high performance computing (HPC) cluster for the College of Biological Sciences at UC Davis. Its primary use is for research in genetics, genomics, and proteomics, structural biology via cryogenic electron microscopy, computational neuroscience, and generally, the computional biology workflows related to those fields. Franklin currently consists of 6 AMD  CPU nodes each with 128 physical and 256 logical cores and 1TB of RAM, 5 GPU nodes with a total of 40 Nvidia  RTX A4000, RTX A5000, and RTX 2080 TI GPUs, and a collection of ZFS  file servers providing over 2PB of storage.</p> <p></p>"},{"location":"#administration","title":"Administration","text":"<p>Franklin is maintained by the HPC Core Facility. Software installation and support requests should be directed to hpc-help@ucdavis.edu.</p> <p></p>"},{"location":"#using-this-documentation","title":"Using This Documentation","text":"<p>Before contacting HPCCF support, first try searching this documentation. This site provides information on accessing and interacting with the cluster, an overview of available software ecosystems, and tutorials for commonly used software and access patterns. It is split into a Users section for end-users and an Admin section with information relevant to system administators. This documentation is being actively expanded as Franklin's software and userbase grows.</p> <p>This site is written in markdown using MkDocs with the Material for MkDocs theme. If you would like to contribute, you may fork our repo  and submit a pull request .</p>"},{"location":"general/access/","title":"Accessing Franklin","text":""},{"location":"general/access/#x11-forwarding","title":"X11 Forwarding","text":"<p>Some software has a Graphical User Interface (GUI), and so requires X11 to be enabled. X11 forwarding allows an application on a remote server (in this case, Franklin) to render its GUI on a local system (your computer). How this is enabled depends on the operating system the computer you are using to access Franklin is running.</p>"},{"location":"general/access/#linux","title":"Linux","text":"<p>If you are SSHing from a Linux distribution, you likely already have an X11 server running locally, and can support forwarding natively. If you are on campus, you can use the <code>-Y</code> flag to enable it, like:</p> <pre><code>$ ssh -Y [USER]@franklin.hpc.ucdavis.edu\n</code></pre> <p>If you are off campus on a slower internet connection, you may get better performance by enabling compression with:</p> <pre><code>$ ssh -Y [USER]@franklin.hpc.ucdavis.edu\n</code></pre>"},{"location":"general/access/#macos","title":"MacOS","text":"<p>MacOS does not come with an X11 implementation out of the box. You will first need to install the free, open-source XQuartz package, after which you can use the same <code>ssh</code> flags as described in the Linux instructions.</p>"},{"location":"general/access/#windows","title":"Windows","text":"<p>If you are using our recommend windows SSH client, MobaXterm, X11 forwarding should be enabled by default. You can confirm this by checking that the <code>X11-Forwarding</code> box is ticked under your Franklin session settings. For off-campus access, you may want to tick the <code>Compression</code> box as well.</p>"},{"location":"scheduler/","title":"Slurm","text":"<p>HPC clusters run job schedulers to distribute and manage computational resources. Generally, schedulers:</p> <ul> <li>Manage and enforce resource constraints, such as execution time, number of CPU cores, and amount of RAM a job may use;</li> <li>Provide tools for efficient communication between nodes during parallel workflows;</li> <li>Fairly coordinate the order and priority of job execution between users;</li> <li>Monitor the status and utilization of nodes.</li> </ul> <p></p> <p>Franklin uses Slurm as its job scheduler. A central controller runs on one of the file servers, which users submit jobs to from the access node using the <code>srun</code> and <code>sbatch</code> commands. The controller then determines a priority for the job based on the resources requested and schedules it on the queue. Priority calculation can be complex, but the overall goal of the scheduler is to optimize a tradeoff between throughput on the cluster as a whole and turnaround time on jobs.</p> <p>The Jobs section describes how to submit and manage jobs with Slurm. The Queueing section describes Franklin's queuing policy and structure. The Status section details how to use Slurm to monitor the status of the cluster as whole.</p>"},{"location":"scheduler/jobs/","title":"Jobs","text":"<p>After logging in to Franklin, your session exists on the head node: a single, less powerful computer that serves as the gatekeeper to the rest of the cluster. To do actual work, you will need to write submission scripts that define your job and submit them to the cluster along with resource requests.</p>"},{"location":"scheduler/jobs/#batch-jobs-sbatch","title":"Batch Jobs: <code>sbatch</code>","text":"<p>Most of the time, you will want to submit jobs in the form of job scripts. The batch job script specifies the resources needed for the job, such as the number of nodes,  cores, memory, and walltime. A simple example would be:</p> jobscript.sh<pre><code>#!/bin/bash \n# (1)\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --time=01:00:00\n#SBATCH --mem=100MB\n#SBATCH --partition=low\n\necho \"Running on $(hostname)\"\n</code></pre> <ol> <li>This will determine the shell Slurm uses to execute your script. You could, for example, use <code>/bin/sh</code> or <code>/bin/zsh</code>.</li> </ol> <p>Which can be submitted to the scheduler by running:</p> <pre><code>$ sbatch jobscript.sh\nSubmitted batch job 629\n</code></pre> <p>The job script is a normal shell script -- note the <code>#!/bin/bash</code> -- that contains additional directives. <code>#SBATCH</code> lines specify directives to be sent to the scheduler; in this case, our resource requests:</p> <ul> <li><code>--ntasks</code>: Number of tasks to run. Slurm may schedule tasks on the same or different nodes.</li> <li><code>--cpus-per-task</code>: Number of CPUs (cores) to allocate per task.</li> <li><code>--time</code>: Maximum wallclock time for the job.</li> <li><code>--mem</code>: Maximum amount of memory for the job.</li> <li><code>--partition</code>: The queue partition to submit to. See the queueing section for more details.</li> </ul> Warning <p>Jobs that exceed their memory or time constraints will be automatically killed. There is no limit on spawning threads, but keep in mind that using far more threads than requested cores will result in rapidly decreasing performance.</p> <p><code>#SBATCH</code> directives directly correspond to arguments passed to the <code>sbatch</code> command. As such, one could remove the lines starting with <code>#SBATCH</code> from the previous job script and submit it with:</p> <pre><code>$ sbatch --ntasks=1 --cpus-per-task=1 --time=01:00:00 --mem=100MB --partition=low jobscript.sh\n</code></pre> <p>Using directives with job scripts is recommended, as it helps you document your resource requests.</p> <p>Try <code>man sbatch</code> or visit the official docs for more options. More information on resource requests can be found in the Resources section, and more examples on writing job scripts can be found in the Job Scripts section.</p>"},{"location":"scheduler/jobs/#interactive-jobs-srun","title":"Interactive jobs: <code>srun</code>","text":"<p>Sometimes, you want to run an interactive shell session on a node, such as running an IPython session. <code>srun</code> takes the same parameters as <code>sbatch</code>, while also allowing you to specify a shell. For example:</p> <pre><code>$ srun --ntasks=1 --time=01:00:00 --mem=100MB --partition=low --pty /bin/bash\nsrun: job 630 queued and waiting for resources\nsrun: job 630 has been allocated resources\ncamw@c-8-42:~$\n</code></pre> <p>Note that addition of the <code>--pty /bin/bash</code> argument. You can see that the job is queued and then allocated resources, but instead of exiting, you are brought to a new prompt. In the example above, the user <code>camw</code> has been moved onto the node <code>c-8-42</code>, which is indicated by the new terminal prompt, <code>camw@c-8-42</code>. The same resource and time constraints apply in this session as in <code>sbatch</code> scripts.</p> Note <p>This is the only way to get direct access to a node: you will not be able to simply do <code>ssh c-8-42</code>, for example.</p> <p>Try <code>man srun</code> or visit the official docs for more options.</p>"},{"location":"scheduler/jobs/#listing-jobs-squeue","title":"Listing jobs: <code>squeue</code>","text":"<p><code>squeue</code> can be used to monitor running and queued jobs. Running it with no arguments will show all the jobs on the cluster; depending on how many users are active, this could be a lot!</p> <pre><code>$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               589 jawdatgrp Refine3D adtaheri  R 1-13:51:39      1 gpu-9-18\n               631       low jobscrip     camw  R       0:19      1 c-8-42\n               627       low Class2D/ mashaduz  R      37:11      1 gpu-9-58\n...\n</code></pre> <p>To view only your jobs, you can use <code>squeue --me</code>.</p> <pre><code>$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               631       low jobscrip     camw  R       0:02      1 c-8-42\n</code></pre> <p>The format -- which columns and their width -- can be tuned with the <code>--format</code> parameter. For example, you might way to also include how many cores the job requested, and widen the fields:</p> <pre><code>$ squeue --format=\"%10i %.9P %.20j %.10u %.3t %.25S %.15L %.10C %.6D %.20R\"\nJOBID      PARTITION                 NAME       USER  ST                START_TIME       TIME_LEFT       CPUS  NODES     NODELIST(REASON)\n589        jawdatgrp     Refine3D/job015/   adtaheri   R       2023-01-31T22:51:59         9:58:38          6      1             gpu-9-18\n627              low      Class2D/job424/   mashaduz   R       2023-02-02T12:06:27        11:13:06         60      1             gpu-9-58\n</code></pre> <p>Try <code>man squeue</code> or visit the official docs for more options.</p>"},{"location":"scheduler/jobs/#canceling-jobs-scancel","title":"Canceling jobs: <code>scancel</code>","text":"<p>To kill a job before it has completed, use the scancel command:</p> <pre><code>$ scancel JOBID # (1)!\n</code></pre> <ol> <li>Replace <code>JOBID</code> with the ID of your job, which can be obtained with <code>squeue</code>.</li> </ol> <p>You can cancel many jobs at a time; for example, you could cancel all of your running jobs with:</p> <pre><code>$ scancel -u $USER #(1)!\n</code></pre> <ol> <li><code>$USER</code> is an environment variable containing your username, so leave this as is to use it.</li> </ol> <p>Try <code>man scancel</code> or visit the official docs for more options.</p>"},{"location":"scheduler/jobs/#job-and-cluster-information-scontrol","title":"Job and Cluster Information:  <code>scontrol</code>","text":"<p><code>scontrol show</code> can be used to display any information known to Slurm. For users, the most useful are the detailed job and node information.</p> <p>To display details for a job, run:</p> <pre><code>$ scontrol show j 635\nJobId=635 JobName=jobscript.sh\n   UserId=camw(1134153) GroupId=camw(1134153) MCS_label=N/A\n   Priority=6563 Nice=0 Account=admin QOS=adminmed\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:24 TimeLimit=01:00:00 TimeMin=N/A\n   SubmitTime=2023-02-02T13:26:24 EligibleTime=2023-02-02T13:26:24\n   AccrueTime=2023-02-02T13:26:24\n   StartTime=2023-02-02T13:26:25 EndTime=2023-02-02T14:26:25 Deadline=N/A\n   PreemptEligibleTime=2023-02-02T13:26:25 PreemptTime=None\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-02-02T13:26:25 Scheduler=Main\n   Partition=low AllocNode:Sid=nas-8-0:449140\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=c-8-42\n   BatchHost=c-8-42\n   NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=2,mem=100M,node=1,billing=2\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=1 MinMemoryNode=100M MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/home/camw/jobscript.sh\n   WorkDir=/home/camw\n   StdErr=/home/camw/slurm-635.out\n   StdIn=/dev/null\n   StdOut=/home/camw/slurm-635.out\n   Power=\n</code></pre> <p>Where <code>635</code> should be replaced with the ID for your job. For example, you can see that this job was allocated resources on <code>c-8-42</code> (<code>NodeList=c-8-42</code>), that its priority score is 6563 (<code>Priority=6563</code>), and that the script it ran with is located at <code>/home/camw/jobscript.sh</code>.</p> <p>We can also get details on nodes. Let's interrogate <code>c-8-42</code>:</p> <pre><code>$ scontrol show n c-8-42\nNodeName=c-8-42 Arch=x86_64 CoresPerSocket=64 \n   CPUAlloc=4 CPUEfctv=256 CPUTot=256 CPULoad=0.12\n   AvailableFeatures=amd,cpu\n   ActiveFeatures=amd,cpu\n   Gres=(null)\n   NodeAddr=c-8-42 NodeHostName=c-8-42 Version=22.05.6\n   OS=Linux 5.15.0-56-generic #62-Ubuntu SMP Tue Nov 22 19:54:14 UTC 2022 \n   RealMemory=1000000 AllocMem=200 FreeMem=98124 Sockets=2 Boards=1\n   State=MIXED ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A\n   Partitions=low,high \n   BootTime=2022-12-11T02:25:44 SlurmdStartTime=2022-12-14T10:34:25\n   LastBusyTime=2023-02-02T13:13:22\n   CfgTRES=cpu=256,mem=1000000M,billing=256\n   AllocTRES=cpu=4,mem=200M\n   CapWatts=n/a\n   CurrentWatts=0 AveWatts=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n</code></pre> <p><code>CPUAlloc=4</code> tells us that 4 cores are currently allocated on the node. <code>AllocMem=200</code> indicates that 200MiB of RAM are currently allocated, with <code>RealMemory=1000000</code> telling us that there is 1TiB of RAM total on the node.</p>"},{"location":"scheduler/jobs/#node-status-sinfo","title":"Node Status: <code>sinfo</code>","text":"<p>Another useful status command is <code>sinfo</code>, which is specialized for displaying information on nodes and partitions. Running it without any arguments gives information on partitions:</p> <pre><code>$ sinfo\nPARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST\nlow*             up   12:00:00      3    mix gpu-9-[10,18,58]\nlow*             up   12:00:00      8   idle c-8-[42,50,58,62,70,74],gpu-9-[26,66]\nhigh             up 60-00:00:0      6   idle c-8-[42,50,58,62,70,74]\njawdatgrp-gpu    up   infinite      2    mix gpu-9-[10,18]\njawdatgrp-gpu    up   infinite      1   idle gpu-9-26\n</code></pre> <p>In this case, we can see that there are 3 partially-allocated nodes in the <code>low</code> partition (they have state <code>mix</code>), and that the time limit for jobs on the <code>low</code> partition is 12 hours.</p> <p>Passing the <code>-N</code> flag tells <code>sinfo</code> to display node-centric information:</p> <pre><code>$ sinfo -N\nNODELIST   NODES     PARTITION STATE \nc-8-42         1          low* idle  \nc-8-42         1          high idle  \nc-8-50         1          low* idle  \nc-8-50         1          high idle  \nc-8-58         1          low* idle  \nc-8-58         1          high idle  \nc-8-62         1          low* idle  \nc-8-62         1          high idle  \nc-8-70         1          low* idle  \nc-8-70         1          high idle  \nc-8-74         1          low* idle  \nc-8-74         1          high idle  \ngpu-9-10       1          low* mix   \ngpu-9-10       1 jawdatgrp-gpu mix   \ngpu-9-18       1          low* mix   \ngpu-9-18       1 jawdatgrp-gpu mix   \ngpu-9-26       1          low* idle  \ngpu-9-26       1 jawdatgrp-gpu idle  \ngpu-9-58       1          low* mix   \ngpu-9-66       1          low* idle\n</code></pre> <p>There is an entry for each node in each of its partitions. <code>c-8-42</code> is in both the <code>low</code> and <code>high</code> partitions, while <code>gpu-9-10</code> is in the <code>low</code> and <code>jawdatgrp-gpu</code> partitions.</p> <p>More verbose information can be obtained by also passing the <code>-l</code> or <code>--long</code> flag:</p> <pre><code>$ sinfo -N -l\nThu Feb 02 14:04:48 2023\nNODELIST   NODES     PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON              \nc-8-42         1          low*        idle 256    2:64:2 100000        0      1  amd,cpu none                \nc-8-42         1          high        idle 256    2:64:2 100000        0      1  amd,cpu none                \nc-8-50         1          low*        idle 256    2:64:2 100000        0      1  amd,cpu none                \nc-8-50         1          high        idle 256    2:64:2 100000        0      1  amd,cpu none                \nc-8-58         1          low*        idle 256    2:64:2 100000        0      1  amd,cpu none\n...\n</code></pre> <p>This view gives the nodes' socket, core, and thread configurations, their RAM, and the feature list, which you can read about in the Resources section. Try <code>man scontrol</code> or <code>man sinfo</code>, or visit the official docs for <code>scontrol</code> and <code>sinfo</code>, for more options.</p>"},{"location":"scheduler/queues/","title":"Queueing","text":""},{"location":"scheduler/queues/#partitions","title":"Partitions","text":""},{"location":"scheduler/queues/#priorities","title":"Priorities","text":""},{"location":"scheduler/queues/#quality-of-service","title":"Quality of Service","text":""},{"location":"scheduler/resources/","title":"Requesting Resources","text":""},{"location":"scheduler/resources/#resource-types","title":"Resource Types","text":""},{"location":"scheduler/resources/#cpus-cores","title":"CPUs / cores","text":"<p>CPUs are the central compute power behind your jobs. Most scientific software supports multiprocessing (multiple instances of an executable with discrete memory resources, possibly but not necessarily communicating with each other), multithreading (multiple paths, or threads, of execution within a process on a node, sharing the same memory resources, but able to execute on different cores), or both. This allows computation to scale with increased numbers of CPUs, allowing bigger datasets to be analyzed.</p> <p>Slurm's CPU management methods are complex and can quickly become confusing. For the purposes of this documentation, we will provide a simplified explanation; those with advanced needs should consult the Slurm documentation.</p> <p>Slurm follows a distinction between its physically resources -- cluster nodes and CPUs or cores on a node -- and virtual resources, or tasks, which specificy how requested physical resources will be grouped and distributed. By default, Slurm will minimize the number of nodes allocated to a job, and attempt to keep the job's CPU requests localized within a node. Tasks group together CPUs (or other resources): CPUs within a task will be kept together on the same node. Different tasks may end up on different nodes, but Slurm will exhaust the CPUs on a given node before splitting tasks between nodes unless specifically requested.</p> <p>A Complication: SMT / Hyperthreading</p> <p>Slurm understands the distinction between physical and logical cores. Most modern CPUs support Simultaneous Multithreading (SMT), which allows multiple independent processes to run on a single physical core. Although each of these is not a full fledged core, they have independent hardware for certain operations, and can greatly improve scalability for some tasks. However, using an individual thread within a single core makes little sense, as it shares hardware with the other SMT threads on its core; so, Slurm will always keep these threads together. In practice, this means if you ask for an odd number of CPUs, your request will be rounded up so as not to split an SMT thread between different job allocations.</p> <p>The primary parameters controlling these are:</p> <ul> <li><code>--cpus-per-task/-c</code>: How many CPUs to request per task. The number of CPUs requested here will always be on the same node. By default, 1.</li> <li><code>--ntasks/-n</code>: The number of tasks to request. By default, 1.</li> <li><code>--nodes/-N</code>: The minimum number of nodes to request, by default, 1.</li> </ul> <p>Let's explore some examples. The simple request would be to ask for 2 CPUs. We will use <code>srun</code> to request resources and then immediately run the <code>nproc</code> command within the allocation to report how many CPUs are available:</p> <pre><code>$ srun -c 2 nproc \nsrun: job 682 queued and waiting for resources\nsrun: job 682 has been allocated resources\n2\n</code></pre> <p>We asked for 2 CPUs per task, and Slurm gave us 2 CPUs and 1 task. What happens if we ask for 2 tasks instead of 2 CPUs?</p> <pre><code>$ srun -n 2 nproc\nsrun: job 683 queued and waiting for resources\nsrun: job 683 has been allocated resources\n1\n1\n</code></pre> <p>This time, we were given 2 separate tasks, each of which got 1 CPU. Each task ran its own instance of the <code>nproc</code> command, and so each reported <code>1</code>. If we ask for more CPUs per task:</p> <pre><code>$ srun -n 2 -c 2 nproc\nsrun: job 684 queued and waiting for resources\nsrun: job 684 has been allocated resources\n2\n2\n</code></pre> <p>We still asked for 2 tasks, but this time we requested 2 CPUs in each. So, we got 2 instances of <code>nproc</code>, each reported <code>2</code> CPUs in their task.</p> <p>Summary</p> <p>If you want to run multithreaded jobs, use <code>--cpus-per-task N_THREADS</code> and <code>-ntasks 1</code>. If you want a multiprocess job (or an MPI job), increase <code>-ntasks</code>.</p> The SMT Edge Case <p>If we use <code>-c 1</code> without specifying the number of tasks, we might be taken by surprise:</p> <pre><code>$ srun -c 1 nproc     \nsrun: job 685 queued and waiting for resources\nsrun: job 685 has been allocated resources\n1\n1\n</code></pre> <p>We only asked for 1 CPU per task, but we got 2 tasks! This is due to SMT, described in the note above. Because Slurm will not split SMT threads, and there are 2 SMT threads per physical core, the request was rounded up to 2 CPUs total. In order to keep with the 1 CPU-per-task constraint, it spawned 2 tasks. Similarly, if we specify that we only want 1 task, CPUs per task will instead be bumped:</p> <pre><code>$ srun -c 1 -n 1 nproc\nsrun: job 686 queued and waiting for resources\nsrun: job 686 has been allocated resources\n2\n</code></pre>"},{"location":"scheduler/resources/#nodes","title":"Nodes","text":"<p>Let's explore multiple nodes a bit further. We have seen previously that the <code>-n/ntasks</code> parameter will allocate discrete groups of cores. In our prior examples, however, we used small resource requests. What happens when we want to distribute jobs across nodes?</p> <p>Slurm uses the block distribution method by default to distribute tasks betwee nodes. It will exhaust all the CPUs on a node with task groups before moving to a new node. For these examples, we're going to create a script that reports both the hostname (ie, the node) and the number of CPUs:</p> host-nprocs.sh<pre><code>#!/bin/bash\n\necho `hostname`: `nproc`\n</code></pre> <p>And make it executable with <code>chmod +x host-nprocs.sh</code>. </p> <p>Now let's make a multiple-task request:</p> <pre><code>$ srun -c 2 -n 2 ./host-nprocs.sh\nsrun: job 691 queued and waiting for resources\nsrun: job 691 has been allocated resources\nc-8-42: 2\nc-8-42: 2\n</code></pre> <p>As before, we asked for 2 tasks and 2 CPUs per task. Both tasks were assigned to <code>c-8-42</code>, because it had enough CPUs to fulfill the request. What if it did not?</p> <pre><code>$ srun -c 120 -n 3 ./host-nprocs.sh\nsrun: job 692 queued and waiting for resources\nsrun: job 692 has been allocated resources\nc-8-42: 120\nc-8-42: 120\nc-8-50: 120\n</code></pre> <p>This time, we asked for 3 tasks each with 120 CPUs. The first two tasks were able to be fulfilled by the node <code>c-8-42</code>, but that node did not have enough CPUs to allocate another 120 on top of that. So, the third task was distributed to <code>c-8-50</code>. Thus, this task spanned multiple nodes.</p> <p>Sometimes, we want to make sure each task has its own node. We can achieve this with the <code>--nodes/-N</code> parameter. This specifies the minimum number of nodes the tasks should be allocated across. If we rerun the above example:</p> <pre><code>$ srun -c 120 -n 3 -N 3 ./host-nprocs.sh\nsrun: job 693 queued and waiting for resources\nsrun: job 693 has been allocated resources\nc-8-42: 120\nc-8-50: 120\nc-8-58: 120\n</code></pre> <p>We still asked for 3 tasks and 3 CPUs per task, but this time we specified we wanted a minimum of 3 nodes. As a result, we were allocated portions of <code>c-8-42</code>, <code>c-8-50</code>, and <code>c-8-58</code>.</p>"},{"location":"scheduler/resources/#ram-memory","title":"RAM / Memory","text":"<p>Random Access Memory (RAM) is the fast, volatile storage that your programs use to store data during execution. This can be contrasted with disk storage, which is non-volatile and many orders of magnitude slower to access, and is used for long term data -- say, your sequencing runs or cryo-EM images. RAM is a limited resource on each node, so Slurm enforces memory limits for jobs using cgroups. If a job step consumes more RAM than requested, the step will be killed.</p> <p>Some (mutually exclusive) parameters for requesting RAM are:</p> <ul> <li><code>--mem</code>: The memory required per-node. Usually, you want to use <code>--mem-per-cpu</code>.</li> <li><code>--mem-per-cpu</code>: The memory required per CPU or core. If you requested \\(N\\) tasks, \\(C\\) CPUs per task, and \\(M\\) memory per CPU, your total memory usage will be \\(N * C * M\\). Note that, if \\(N \\gt 1\\), you will have \\(N\\) discrete \\(C * M\\)-sized chunks of RAM requested, possibly across different nodes.</li> <li><code>--mem-per-gpu</code>: Memory required per GPU, which will scale with GPUs in the same way as <code>--mem-per-cpu</code> will with CPUs.</li> </ul> <p>For all memory requests, units can be specified explicitly with the suffixes <code>[K|M|G|T]</code> for <code>[kilobytes|megabytes|gigabytes|terabytes]</code>, with the default units being <code>M</code>/<code>megabytes</code>. So, <code>--mem-per-cpu=500</code> will requested 500 megabytes of RAM per CPU, and <code>--mem-per-cpu=32G</code> will request 32 gigabytes of RAM per CPU.</p> <p>Here is an example of a task overrunning its memory allocation. We will use the <code>stress-ng</code> program to allocate 8 gigabytes of RAM in a job that only requested 200 megabytes.</p> <pre><code>$ srun -n 1 --cpus-per-task 2 --mem-per-cpu 200M stress-ng -m 1 --vm-bytes 8G --oomable         1 \u21b5\nsrun: job 706 queued and waiting for resources\nsrun: job 706 has been allocated resources\nstress-ng: info:  [3037475] defaulting to a 86400 second (1 day, 0.00 secs) run per stressor\nstress-ng: info:  [3037475] dispatching hogs: 1 vm\nstress-ng: info:  [3037475] successful run completed in 2.23s\nslurmstepd: error: Detected 1 oom-kill event(s) in StepId=706.0. Some of your processes may have been killed by the cgroup out-of-memory handler.\nsrun: error: c-8-42: task 0: Out Of Memory\nsrun: launch/slurm: _step_signal: Terminating StepId=706.0\n</code></pre>"},{"location":"scheduler/resources/#gpus-gres","title":"GPUs / GRES","text":""},{"location":"software/conda/","title":"Python and Conda","text":""},{"location":"software/cryoem/","title":"Cryo-EM Software Stack","text":""},{"location":"software/cryoem/#relion","title":"Relion","text":"<p>Franklin has multiple CPU and GPU optimized versions of the Relion cryo-EM structural determination package. The head node has been configured to support X11 forwarding, enabling the Relion GUI to be launched. Relion jobs are submitted for batch processing on the cluster node via Slurm. Each Relion module exports the necessary configurations to pre-fill job submission and dependency information in the GUI, and we have defined additional GUI fields to further configure Slurm parameters. We are also maintaining an additional software package, <code>relion-helper</code>, to assist users in switching between Relion modules within the same project.</p> <p>Your first step is deciding which Relion variant you should use. We recommend version 4.0.0, as it is the currently-supported stable release. There are three variants of this version: <code>relion/cpu/4.0.0+amd</code>, <code>relion/gpu/4.0.0+amd</code>, and <code>relion/gpu/4.0.0+intel</code>, which correspond to the CPU optimized, GPU with AMD CPU optimized, and GPU with Intel CPU optimized builds, respectively. More information about these modules is available in the Module Variants section. In general, unless you have access to the three GPU nodes owned by the Al-Bassam lab, you can ignore the Intel variants, and use the CPU <code>+amd</code> version for multi-node CPU only jobs and the GPU <code>+amd</code> version if you have access to a GPU node.</p> <p>If you are completely unfamiliar with Relion, you should start with the tutorial.</p> <p>Note</p> <p>Because Relion is GUI driven, you need to <code>ssh</code> to Franklin with X11 forwarding enabled. Instructions for enabling X11 forwarding can be found in the Access section.</p>"},{"location":"software/cryoem/#launching-relion","title":"Launching Relion","text":"<p>Make sure you have loaded one of the Relion modules:</p> <pre><code>$ module list relion\n\nCurrently Loaded Modules Matching: relion\n  1) relion/gpu/4.0.0+amd   2) relion-helper/0.2\n</code></pre> <p>Change your working directory your Relion project directory and type <code>relion</code>. The Relion GUI should then pop up locally. There will be a bit of latency when using it, especially if you are off campus. You may be able to reduce latency by enabling SSH compression.</p> <p> </p> The relion start screen."},{"location":"software/cryoem/#dependency-configuration","title":"Dependency Configuration","text":"<p>The paths to software that different Relion jobs use will be automatically filled in. Editing these paths, unless you really, really know what you are doing, is not recommended and will likely result in problems, as some of these dependencies are compiled with architecture-specific flags that match their Relion variant.</p> <p> </p> Pre-filled dependent program path. <p>Danger</p> <p>If you plan to switch between Relion modules within the same project, you must use the procedure described in the relion-helper section. Failure to do so will result in execution errors.</p>"},{"location":"software/cryoem/#slurm-configuration","title":"Slurm Configuration","text":"<p>Our Relion deployment has additional fields in the Running tabs. These new fields are:</p> <ul> <li>Email: The email to which Slurm will send job status updates. Fills the <code>--mail-user</code> <code>sbatch</code>/<code>srun</code> parameter.</li> <li>Memory per CPU: Fills the Slurm <code>--memory-per-cpu</code> parameter. Total RAM use of a job will be (Number of MPI procs) * (Number of Threads) * (Memory per CPU), when the Number of Threads field is available; otherwise it will be  (Number of MPI procs) * (Memory per CPU).</li> <li>Job Time: Fills Slurm's <code>--time</code> parameter.</li> <li>GPU Resources: Only available in the GPU modules. Number (and optionally type) of GPUs to request for this job. If only an integer is supplied, will request any GPU. If <code>TYPE:NUM</code> is supplied (example: <code>a4000:4</code>), specific models of GPU will be requested. See the Resources section for more information on available GPU types.</li> </ul> CPU BuildGPU Build <p> The <code>relion/cpu</code> modules lack the GPU resources field. Note the submission script as well. </p> <p> The <code>relion/gpu</code> module has an extra field for GPU resources. Also note the differing submission script. </p> <p>The default GUI fields serve their original purposes:</p> <ul> <li>Number of MPI procs: This will fill the Slurm <code>--ntasks</code> parameter. These tasks may be distributed across multiple nodes, depending on the number of Threads requested. For GPU runs, this should be the number of GPUs + 1.</li> <li>Number of Threads: The will fill the Slurm <code>--cpus-per-task</code> parameter, which means it is the number of threads per MPI proc. Some job types do not expose this field, as they can only be run with a single-thread per MPI proc.</li> <li>Queue name: The Slurm partition to submit to, filling the <code>--partition</code> parameter. More information on partitions can be found in the Queueing section.</li> <li>Standard submission script: The location of the Slurm job script template that will be used. This field will be filled with the appropriate template for the loaded Relion module by default, and should not be changed.For advanced users only: if you are familiar with Relion and want to further fine-tune your Slurm scripts, you can write your own based on the provided templates found in <code>/share/apps/spack/templates/hpccf/franklin</code> or in our spack GitHub repo.</li> <li>Minimum dedicated cores per node: Unused on our system.</li> </ul>"},{"location":"software/cryoem/#switching-between-relion-modules-relion-helper","title":"Switching Between Relion Modules: relion-helper","text":"<p>Sometimes, you may wish to use different Relion modules for different tasks while working within the same project --  perhaps you'd prefer to use the CPU-optimized version for CTF estimation and the GPU-optimized version for 3D refinement. This does not work out of the box. Relion fills the filesystem paths of its dependencies and templates from environment variables, and those environment variables are set in the modulefiles of the differing Relion builds. However, when a Relion job is run, those paths are cached in hidden <code>.star</code> files in the project directory, and the next time Relion is run, it fills those paths from the cache files instead of the environment variables. This means that, after switching modules, the cached location of the previous module will be used, instead of the exported environment variables from the new module. This causes major breakage due to dependencies having different compilation options to match the parent Relion they are attached to and Slurm templates having different configuration options available.</p> <p>Luckily, we have a solution! We wrote and are maintaining relion-helper, a simple utility that updates the cached variables in a project to match whatever Relion module is currently loaded. Let's go over example use of the tool.</p> <p>In this example, assume we have a relion project directory at <code>/path/to/my/project</code>. We ran some steps with the module <code>relion/gpu/4.0.0+amd</code>, and now want to switch to <code>relion/cpu/4.0.0+amd</code>. First, let's swap modules:</p> <pre><code>$ module unload relion/gpu/4.0.0+amd \namdfftw/3.2+amd: unloaded.\nctffind/4.1.14+amd: unloaded.\nrelion/gpu/4.0.0+amd: unloaded.\nmotioncor2/1.5.0: unloaded.\ngctf/1.06: unloaded.\nghostscript/9.56.1: unloaded.\n\n$ module load relion/cpu/4.0.0+amd.lua \namdfftw/3.2+amd: loaded.\nctffind/4.1.14+amd: loaded.\nrelion/cpu/4.0.0+amd: loaded.\nmotioncor2/1.5.0: loaded.\ngctf/1.06: loaded.\nghostscript/9.56.1: loaded.\n</code></pre> <p>And load relion-helper:</p> <pre><code>$ module load relion-helper \nrelion-helper/0.2: loaded.\n\n$ relion-helper -h\nusage: relion-helper [-h] {reset-cache} ...\n\npositional arguments:\n  {reset-cache}\n\noptions:\n  -h, --help     show this help message and exit\n</code></pre> <p>Now, change to the project directory:</p> <pre><code>$ cd /path/to/my/project\n</code></pre> <p>Then, run the utility.  It will pull the updated values from the appropriate environment variables that were exported by the new module and write them to the cache files in-place.</p> <pre><code>$ relion-helper reset-cache\n&gt; .gui_ctffindjob.star:41:\n  qsub_extra2: 2 =&gt; 10000\n&gt; .gui_ctffindjob.star:42:\n  qsub_extra3: 10000 =&gt; 12:00:00\n&gt; .gui_ctffindjob.star:43:\n  qsubscript: /share/apps/spack/templates/hpccf/franklin/relion.4.0.0.gpu.zen2.slurm.template.sh =&gt; \n/share/apps/spack/templates/hpccf/franklin/relion.4.0.0.cpu.slurm.template.sh\n&gt; .gui_class2djob.star:53:\n  qsub_extra2: 2 =&gt; 10000\n&gt; .gui_class2djob.star:54:\n  qsub_extra3: 10000 =&gt; 12:00:00\n&gt; .gui_class2djob.star:55:\n  qsubscript: /share/apps/spack/templates/hpccf/franklin/relion.4.0.0.gpu.zen2.slurm.template.sh =&gt; \n/share/apps/spack/templates/hpccf/franklin/relion.4.0.0.cpu.slurm.template.sh\n&gt; .gui_autopickjob.star:63:\n  qsub_extra2: 2 =&gt; 10000\n&gt; .gui_autopickjob.star:64:\n  qsub_extra3: 10000 =&gt; 12:00:00\n&gt; .gui_autopickjob.star:65:\n  qsubscript: /share/apps/spack/templates/hpccf/franklin/relion.4.0.0.gpu.zen2.slurm.template.sh =&gt; \n/share/apps/spack/templates/hpccf/franklin/relion.4.0.0.cpu.slurm.template.sh\n&gt; .gui_importjob.star:38:\n  qsub_extra2: 2 =&gt; 10000\n...\n</code></pre> <p>The above output is truncated for brevity. For each cached variable it updates, it reports the name of the cache file, the line number of the change, and the  variable name and value of the change. You can now launch Relion and continue with your work.</p> <p>Each time you want to switch Relion modules for a project, you will need to run this after loading the new module.</p> <p>For now, relion-helper only has the <code>reset-cache</code> subcommand. You can skip <code>cd</code>ing to the project directory by passing the project directory to it instead:</p> <pre><code>$ relion-helper reset-cache -p /path/to/my/project\n</code></pre> <p>Although the changes are made in-place, it leaves backups of the modified files, in case you are concerned about bugs. The original files are of the form <code>.gui_[JOBNAME].star</code>, and the backups are suffixed with <code>.bak</code>:</p> <pre><code>$ ls -al /path/to/my/project\ntotal 317\ndrwxrwxr-x 10 camw camw   31 Feb  3 10:02 .\ndrwxrwxr-x  4 camw camw    6 Jan 12 12:58 ..\ndrwxrwxr-x  5 camw camw    5 Jan 12 12:46 .Nodes\ndrwxrwxr-x  2 camw camw    2 Jan 12 12:40 .TMP_runfiles\n-rw-rw-r--  1 camw camw 1959 Feb  3 10:02 .gui_autopickjob.star\n-rw-rw-r--  1 camw camw 1957 Feb  3 10:01 .gui_autopickjob.star.bak\n-rw-rw-r--  1 camw camw 1427 Feb  3 10:02 .gui_class2djob.star\n-rw-rw-r--  1 camw camw 1425 Feb  3 10:01 .gui_class2djob.star.bak\n-rw-rw-r--  1 camw camw 1430 Feb  3 10:02 .gui_ctffindjob.star\n-rw-rw-r--  1 camw camw 1428 Feb  3 10:01 .gui_ctffindjob.star.bak\n...\n</code></pre> <p>Warning</p> <p>We do not recommend changing between major Relion versions within the same project: ie, from 3.0.1 to 4.0.0.</p>"},{"location":"software/cryoem/#module-variants","title":"Module Variants","text":"<p>There are currently six variations of Relion available on Franklin. Versions 3.1.3 and 4.0.0 are available, each with:</p> <ul> <li>A CPU-optimized build compiled for AMD processors: <code>relion/cpu/[VERSION]+amd</code></li> <li>A GPU-optimized build compiled for AMD processors: <code>relion/gpu/[VERSION]+amd</code></li> <li>A GPU-optimized build compiled for Intel processors: <code>relion/gpu/[VERSION]+intel</code></li> </ul> <p>The CPU-optimized builds were configured with <code>-DALTCPU=True</code> and without CUDA support. For Relion CPU jobs, they will be much faster than the GPU variants. The AMD-optimized <code>+amd</code> variants were compiled with <code>-DAMDFFTW=ON</code> and linked against the <code>amdfftw</code> implementation of  <code>FFTW</code>, in addition to having Zen 2 microarchitecture flags specified to GCC. The <code>+intel</code> variants were compiled with AVX2 support and configured with the <code>-DMKLFFT=True</code> flag, so they use the Intel OneAPI MKL implementation of <code>FFTW</code>. All the GPU variants are targeted to a CUDA compute version of 7.5. The full Cryo-EM software stack is defined in the HPCCF spack configuration repository, and we maintain our own Relion spack package definition. More information on the configurations described here can be found in the Relion docs.</p> <p>The different modules may need to be used with different Slurm resource directives, depending on their variants. The necessary directives, given a module and job partition, are as follows:</p> Module Name Slurm Partition Slurm Directives <code>relion/cpu/[3.1.3,4.0.0]+amd</code> <code>low</code> <code>--constraint=amd</code> <code>relion/cpu/[3.1.3,4.0.0]+amd</code> <code>high</code> N/A <code>relion/gpu/[3.1.3,4.0.0]+amd</code> <code>low</code> <code>--constraint=amd --gres=gpu:[$N_GPUs]</code> or <code>--gres=gpu:[a4000,a5000]:[$N_GPUs]</code> <code>relion/gpu/[3.1.3,4.0.0]+amd</code> <code>jalettsgrp-gpu</code> <code>--gres=gpu:[$N_GPUs]</code> <code>relion/gpu/[3.1.3,4.0.0]+amd</code> <code>mmgdept-gpu</code> <code>--gres=gpu:[$N_GPUs]</code> <code>relion/gpu/[3.1.3,4.0.0]+intel</code> <code>low</code> <code>--constraint=intel --gres=gpu:[$N_GPUs]</code> or <code>--gres=gpu:[rtx_2080_ti]:[$N_GPUs]</code> <code>relion/gpu/[3.1.3,4.0.0]+intel</code> <code>jawdatgrp-gpu</code> <code>--gres=gpu:[$N_GPUs]</code> <p>For example, to use the CPU-optimized Relion module <code>relion/cpu/4.0.0+amd</code> on the free, preemptable <code>low</code> partition, you should submit jobs with <code>--constraint=amd</code> so as to eliminate the Intel nodes in that partition from consideration. However, if you have access to and are using the <code>high</code> partition with the same module, no additional Slurm directives are required, as the <code>high</code> partition only has CPU compute nodes. Alternatively, if you were using an AMD-optimized GPU version, like <code>relion/gpu/4.0.0+amd</code>, and wished to use 2 GPUs on the <code>low</code> partition, you would need to provide both the <code>--constraint=amd</code> and a <code>--gres=gpu:2</code> directive, in order to get an AMD node on the partition along with the required GPUs. Those with access to and submitting to the <code>mmgdept-gpu</code> queue would need only to specify <code>--gres=gpu:2</code>, as that partition only has AMD nodes in it.</p> <p>Note</p> <p>If you are submitting jobs via the GUI, these Slurm directives will already be taken care of for you. If you wish to submit jobs manually, you can get the path to Slurm submission template for the currently-loaded module from the <code>$RELION_QSUB_TEMPLATE</code> environment variable; copying this template is a good starting place for building your batch scripts.</p>"},{"location":"software/cryoem/#ctffind","title":"ctffind","text":"<p>Our installation of CTFFIND4 has <code>+amd</code> and <code>+intel</code> variants which, like Relion, are linked against <code>amdfftw</code> and Intel OneAPI MKL, respectively. The Slurm <code>--constraint</code> flags should be used with these as well, when appropriate, as indicated in the Relion directive table. Each Relion module has its companion CTFFIND4 module as a dependency, so the appropriate version will automatically be loaded when you load Relion, and the proper environment variables are set for the Relion GUI to point at them.</p>"},{"location":"software/cryoem/#motioncor2","title":"MotionCor2","text":"<p>We have deployed MotionCor2 binaries which have been patched to link against the appropriate version of CUDA. These are targetted at a generic architecture, as the source code is not available. Like CTFFIND4, this module is brought in by Relion and the proper environment variables set for Relion to use it.</p>"},{"location":"software/cryoem/#gctf","title":"Gctf","text":"<p>Gctf binaries have been patched and deployed in the same manner as MotionCor2.</p>"},{"location":"software/developing/","title":"Developing Software on Franklin","text":""},{"location":"software/list/","title":"Spack-managed Modules","text":"<p>These modules are built and managed by our Spack deployment. Most were compiled for generic architecture, meaning they can run on any node, but some are Intel or AMD specific, and some require GPU support.</p>"},{"location":"software/list/#r","title":"R","text":"<p>R is 'GNU S', a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques: linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering, etc. Please consult the R project homepage for further information.</p> <p>Versions: 4.1.1</p> <p>Arches: generic</p> <p>Modules: <code>R/4.1.1</code></p>"},{"location":"software/list/#abyss","title":"abyss","text":"<p>ABySS is a de novo, parallel, paired-end sequence assembler that is designed for short reads. The single-processor version is useful for assembling genomes up to 100 Mbases in size.</p> <p>Versions: 2.3.1</p> <p>Arches: generic</p> <p>Modules: <code>abyss/2.3.1</code></p>"},{"location":"software/list/#amdfftw","title":"amdfftw","text":"<p>FFTW (AMD Optimized version) is a comprehensive collection of fast C routines for computing the Discrete Fourier Transform (DFT) and various special cases thereof. It is an open-source implementation of the Fast Fourier transform algorithm. It can compute transforms of real and complex-values arrays of arbitrary size and dimension. AMD Optimized FFTW is the optimized FFTW implementation targeted for AMD CPUs. For single precision build, please use precision value as float. Example : spack install amdfftw precision=float</p> <p>Versions: 3.2</p> <p>Arches: amd</p> <p>Modules: <code>amdfftw/3.2+amd</code></p>"},{"location":"software/list/#ant","title":"ant","text":"<p>Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other</p> <p>Versions: 1.10.7</p> <p>Arches: generic</p> <p>Modules: <code>ant/1.10.7</code></p>"},{"location":"software/list/#aragorn","title":"aragorn","text":"<p>ARAGORN, a program to detect tRNA genes and tmRNA genes in nucleotide sequences.</p> <p>Versions: 1.2.38</p> <p>Arches: generic</p> <p>Modules: <code>aragorn/1.2.38</code></p>"},{"location":"software/list/#bedtools2","title":"bedtools2","text":"<p>Collectively, the bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome.</p> <p>Versions: 2.30.0</p> <p>Arches: generic</p> <p>Modules: <code>bedtools2/2.30.0</code></p>"},{"location":"software/list/#blast-plus","title":"blast-plus","text":"<p>Basic Local Alignment Search Tool.</p> <p>Versions: 2.12.0</p> <p>Arches: generic</p> <p>Modules: <code>blast-plus/2.12.0</code></p>"},{"location":"software/list/#blast2go","title":"blast2go","text":"<p>Blast2GO is a bioinformatics platform for high-quality functional annotation and analysis of genomic datasets.</p> <p>Versions: 5.2.5</p> <p>Arches: generic</p> <p>Modules: <code>blast2go/5.2.5</code></p>"},{"location":"software/list/#blat","title":"blat","text":"<p>BLAT (BLAST-like alignment tool) is a pairwise sequence alignment algorithm.</p> <p>Versions: 35</p> <p>Arches: generic</p> <p>Modules: <code>blat/35</code></p>"},{"location":"software/list/#bowtie","title":"bowtie","text":"<p>Bowtie is an ultrafast, memory-efficient short read aligner for short DNA sequences (reads) from next-gen sequencers.</p> <p>Versions: 1.3.0</p> <p>Arches: generic</p> <p>Modules: <code>bowtie/1.3.0</code></p>"},{"location":"software/list/#bowtie2","title":"bowtie2","text":"<p>Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences</p> <p>Versions: 2.4.2</p> <p>Arches: generic</p> <p>Modules: <code>bowtie2/2.4.2</code></p>"},{"location":"software/list/#bwa","title":"bwa","text":"<p>Burrow-Wheeler Aligner for pairwise alignment between DNA sequences.</p> <p>Versions: 0.7.17</p> <p>Arches: generic</p> <p>Modules: <code>bwa/0.7.17</code></p>"},{"location":"software/list/#bwtool","title":"bwtool","text":"<p>bwtool is a command-line utility for bigWig files.</p> <p>Versions: 1.0</p> <p>Arches: generic</p> <p>Modules: <code>bwtool/1.0</code></p>"},{"location":"software/list/#canu","title":"canu","text":"<p>A single molecule sequence assembler for genomes large and small.</p> <p>Versions: 2.2</p> <p>Arches: generic</p> <p>Modules: <code>canu/2.2</code></p>"},{"location":"software/list/#cap3","title":"cap3","text":"<p>CAP3 is DNA Sequence Assembly Program</p> <p>Versions: 2015-02-11</p> <p>Arches: generic</p> <p>Modules: <code>cap3/2015-02-11</code></p>"},{"location":"software/list/#clustal-omega","title":"clustal-omega","text":"<p>Clustal Omega: the last alignment program you'll ever need.</p> <p>Versions: 1.2.4</p> <p>Arches: generic</p> <p>Modules: <code>clustal-omega/1.2.4</code></p>"},{"location":"software/list/#clustalw","title":"clustalw","text":"<p>Multiple alignment of nucleic acid and protein sequences.</p> <p>Versions: 2.1</p> <p>Arches: generic</p> <p>Modules: <code>clustalw/2.1</code></p>"},{"location":"software/list/#corset","title":"corset","text":"<p>Corset is a command-line software program to go from a de novo transcriptome assembly to gene-level counts.</p> <p>Versions: 1.09</p> <p>Arches: generic</p> <p>Modules: <code>corset/1.09</code></p>"},{"location":"software/list/#ctffind","title":"ctffind","text":"<p>Fast and accurate defocus estimation from electron micrographs.</p> <p>Versions: 4.1.14</p> <p>Arches: amd, intel</p> <p>Modules: <code>ctffind/4.1.14+intel</code>, <code>ctffind/4.1.14+amd</code></p>"},{"location":"software/list/#cuda","title":"cuda","text":"<p>CUDA is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU). Note: This package does not currently install the drivers necessary to run CUDA. These will need to be installed manually. See: https://docs.nvidia.com/cuda/ for details.</p> <p>Versions: 11.7.1, 11.8.0</p> <p>Arches: generic</p> <p>Modules: <code>cuda/11.8.0</code>, <code>cuda/11.7.1</code></p>"},{"location":"software/list/#cufflinks","title":"cufflinks","text":"<p>Cufflinks assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples.</p> <p>Versions: 2.2.1</p> <p>Arches: generic</p> <p>Modules: <code>cufflinks/2.2.1</code></p>"},{"location":"software/list/#ea-utils","title":"ea-utils","text":"<p>Command-line tools for processing biological sequencing data. Barcode demultiplexing, adapter trimming, etc. Primarily written to support an Illumina based pipeline - but should work with any FASTQs.</p> <p>Versions: 2021-10-20</p> <p>Arches: generic</p> <p>Modules: <code>ea-utils/2021-10-20</code></p>"},{"location":"software/list/#emboss","title":"emboss","text":"<p>EMBOSS is a free Open Source software analysis package specially developed for the needs of the molecular biology (e.g. EMBnet) user community</p> <p>Versions: 6.6.0</p> <p>Arches: generic</p> <p>Modules: <code>emboss/6.6.0</code></p>"},{"location":"software/list/#exonerate","title":"exonerate","text":"<p>Pairwise sequence alignment of DNA and proteins</p> <p>Versions: 2.4.0</p> <p>Arches: generic</p> <p>Modules: <code>exonerate/2.4.0</code></p>"},{"location":"software/list/#exonerate-gff3","title":"exonerate-gff3","text":"<p>This is an exonerate fork with added gff3 support. Original website with user guides: http://www.ebi.ac.uk/~guy/exonerate/</p> <p>Versions: 2.3.0</p> <p>Arches: generic</p> <p>Modules: <code>exonerate-gff3/2.3.0</code></p>"},{"location":"software/list/#fastqc","title":"fastqc","text":"<p>A quality control tool for high throughput sequence data.</p> <p>Versions: 0.11.9</p> <p>Arches: generic</p> <p>Modules: <code>fastqc/0.11.9</code></p>"},{"location":"software/list/#fftw","title":"fftw","text":"<p>FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST). We believe that FFTW, which is free software, should become the FFT library of choice for most applications.</p> <p>Versions: 3.3.10</p> <p>Arches: generic</p> <p>Modules: <code>fftw/3.3.10</code></p>"},{"location":"software/list/#freebayes","title":"freebayes","text":"<p>Bayesian haplotype-based genetic polymorphism discovery and genotyping.</p> <p>Versions: 1.3.6</p> <p>Arches: generic</p> <p>Modules: <code>freebayes/1.3.6</code></p>"},{"location":"software/list/#gatk","title":"gatk","text":"<p>Genome Analysis Toolkit Variant Discovery in High-Throughput Sequencing Data</p> <p>Versions: 4.2.6.1, 3.8.1</p> <p>Arches: generic</p> <p>Modules: <code>gatk/3.8.1</code>, <code>gatk/4.2.6.1</code></p>"},{"location":"software/list/#gcc","title":"gcc","text":"<p>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages.</p> <p>Versions: 4.9.4, 5.5.0, 7.5.0</p> <p>Arches: generic</p> <p>Modules: <code>gcc/5.5.0</code>, <code>gcc/7.5.0</code>, <code>gcc/4.9.4</code></p>"},{"location":"software/list/#gctf","title":"gctf","text":"<p>a GPU accelerated program for Real-Time CTF determination, refinement, evaluation and correction.</p> <p>Versions: 1.06</p> <p>Arches: generic</p> <p>Modules: <code>gctf/1.06</code></p>"},{"location":"software/list/#genrich","title":"genrich","text":"<p>Genrich is a peak-caller for genomic enrichment assays.</p> <p>Versions: 0.6</p> <p>Arches: generic</p> <p>Modules: <code>genrich/0.6</code></p>"},{"location":"software/list/#ghostscript","title":"ghostscript","text":"<p>An interpreter for the PostScript language and for PDF.</p> <p>Versions: 9.56.1</p> <p>Arches: generic</p> <p>Modules: <code>ghostscript/9.56.1</code></p>"},{"location":"software/list/#glimmer","title":"glimmer","text":"<p>Glimmer is a system for finding genes in microbial DNA, especially the genomes of bacteria, archaea, and viruses.</p> <p>Versions: 3.02b</p> <p>Arches: generic</p> <p>Modules: <code>glimmer/3.02b</code></p>"},{"location":"software/list/#hdf5","title":"hdf5","text":"<p>HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data.</p> <p>Versions: 1.12.2</p> <p>Arches: generic</p> <p>Modules: <code>hdf5/1.12.2</code></p>"},{"location":"software/list/#hisat2","title":"hisat2","text":"<p>HISAT2 is a fast and sensitive alignment program for mapping next- generation sequencing reads (whole-genome, transcriptome, and exome sequencing data) against the general human population (as well as against a single reference genome).</p> <p>Versions: 2.2.0</p> <p>Arches: generic</p> <p>Modules: <code>hisat2/2.2.0</code></p>"},{"location":"software/list/#hmmer","title":"hmmer","text":"<p>HMMER is used for searching sequence databases for sequence homologs, and for making sequence alignments. It implements methods using probabilistic models called profile hidden Markov models (profile HMMs).</p> <p>Versions: 3.3.2</p> <p>Arches: generic</p> <p>Modules: <code>hmmer/3.3.2</code></p>"},{"location":"software/list/#homer","title":"homer","text":"<p>Software for motif discovery and next generation sequencing analysis</p> <p>Versions: 4.9.1</p> <p>Arches: generic</p> <p>Modules: <code>homer/4.9.1</code></p>"},{"location":"software/list/#hwloc","title":"hwloc","text":"<p>The Hardware Locality (hwloc) software project. The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently.</p> <p>Versions: 2.8.0</p> <p>Arches: generic</p> <p>Modules: <code>hwloc/2.8.0</code></p>"},{"location":"software/list/#igv","title":"igv","text":"<p>The Integrative Genomics Viewer (IGV) is a high-performance visualization tool for interactive exploration of large, integrated genomic datasets. It supports a wide variety of data types, including array-based and next-generation sequence data, and genomic annotations.</p> <p>Versions: 2.12.3</p> <p>Arches: generic</p> <p>Modules: <code>igv/2.12.3</code></p>"},{"location":"software/list/#infernal","title":"infernal","text":"<p>Infernal (INFERence of RNA ALignment) is for searching DNA sequence databases for RNA structure and sequence similarities. It is an implementation of a special case of profile stochastic context-free grammars called covariance models (CMs).</p> <p>Versions: 1.1.4</p> <p>Arches: generic</p> <p>Modules: <code>infernal/1.1.4</code></p>"},{"location":"software/list/#intel-oneapi-compilers","title":"intel-oneapi-compilers","text":"<p>Intel oneAPI Compilers. Includes: icc, icpc, ifort, icx, icpx, ifx, and dpcpp. LICENSE INFORMATION: By downloading and using this software, you agree to the terms and conditions of the software license agreements at https://intel.ly/393CijO.</p> <p>Versions: 2022.2.1</p> <p>Arches: generic</p> <p>Modules: <code>intel-oneapi-compilers/2022.2.1</code></p>"},{"location":"software/list/#intel-oneapi-mkl","title":"intel-oneapi-mkl","text":"<p>Intel oneAPI Math Kernel Library (Intel oneMKL; formerly Intel Math Kernel Library or Intel MKL), is a library of optimized math routines for science, engineering, and financial applications. Core math functions include BLAS, LAPACK, ScaLAPACK, sparse solvers, fast Fourier transforms, and vector math. LICENSE INFORMATION: By downloading and using this software, you agree to the terms and conditions of the software license agreements at https://intel.ly/393CijO.</p> <p>Versions: 2022.2.1</p> <p>Arches: generic</p> <p>Modules: <code>intel-oneapi-mkl/2022.2.1</code></p>"},{"location":"software/list/#interproscan","title":"interproscan","text":"<p>InterProScan is the software package that allows sequences (protein and nucleic) to be scanned against InterPro's signatures. Signatures are predictive models, provided by several different databases, that make up the InterPro consortium.</p> <p>Versions: 5.56-89.0</p> <p>Arches: generic</p> <p>Modules: <code>interproscan/5.56-89.0</code></p>"},{"location":"software/list/#iq-tree","title":"iq-tree","text":"<p>IQ-TREE Efficient software for phylogenomic inference</p> <p>Versions: 2.1.3</p> <p>Arches: generic</p> <p>Modules: <code>iq-tree/2.1.3</code></p>"},{"location":"software/list/#iqtree2","title":"iqtree2","text":"<p>Efficient and versatile phylogenomic software by maximum likelihood</p> <p>Versions: 2.1.2</p> <p>Arches: generic</p> <p>Modules: <code>iqtree2/2.1.2</code></p>"},{"location":"software/list/#jdk","title":"jdk","text":"<p>The Java Development Kit (JDK) released by Oracle Corporation in the form of a binary product aimed at Java developers. Includes a complete JRE plus tools for developing, debugging, and monitoring Java applications.</p> <p>Versions: 17.0.1</p> <p>Arches: generic</p> <p>Modules: <code>jdk/17.0.1</code></p>"},{"location":"software/list/#jellyfish","title":"jellyfish","text":"<p>JELLYFISH is a tool for fast, memory-efficient counting of k-mers in DNA.</p> <p>Versions: 1.1.11</p> <p>Arches: generic</p> <p>Modules: <code>jellyfish/1.1.11</code></p>"},{"location":"software/list/#kalign","title":"kalign","text":"<p>A fast multiple sequence alignment program for biological sequences.</p> <p>Versions: 3.3.1</p> <p>Arches: generic</p> <p>Modules: <code>kalign/3.3.1</code></p>"},{"location":"software/list/#kallisto","title":"kallisto","text":"<p>kallisto is a program for quantifying abundances of transcripts from RNA-Seq data.</p> <p>Versions: 0.48.0</p> <p>Arches: generic</p> <p>Modules: <code>kallisto/0.48.0</code></p>"},{"location":"software/list/#kmergenie","title":"kmergenie","text":"<p>KmerGenie estimates the best k-mer length for genome de novo assembly.</p> <p>Versions: 1.7044</p> <p>Arches: generic</p> <p>Modules: <code>kmergenie/1.7044</code></p>"},{"location":"software/list/#kraken","title":"kraken","text":"<p>Kraken is a system for assigning taxonomic labels to short DNA sequences, usually obtained through metagenomic studies.</p> <p>Versions: 1.0</p> <p>Arches: generic</p> <p>Modules: <code>kraken/1.0</code></p>"},{"location":"software/list/#kraken2","title":"kraken2","text":"<p>Kraken2 is a system for assigning taxonomic labels to short DNA sequences, usually obtained through metagenomic studies.</p> <p>Versions: 2.1.2</p> <p>Arches: generic</p> <p>Modules: <code>kraken2/2.1.2</code></p>"},{"location":"software/list/#krakenuniq","title":"krakenuniq","text":"<p>Metagenomics classifier with unique k-mer counting for more specific results.</p> <p>Versions: 0.7.3</p> <p>Arches: generic</p> <p>Modules: <code>krakenuniq/0.7.3</code></p>"},{"location":"software/list/#last","title":"last","text":"<p>LAST finds similar regions between sequences, and aligns them. It is designed for comparing large datasets to each other (e.g. vertebrate genomes and/or large numbers of DNA reads).</p> <p>Versions: 1282</p> <p>Arches: generic</p> <p>Modules: <code>last/1282</code></p>"},{"location":"software/list/#libevent","title":"libevent","text":"<p>The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts.</p> <p>Versions: 2.1.12</p> <p>Arches: generic</p> <p>Modules: <code>libevent/2.1.12</code></p>"},{"location":"software/list/#mash","title":"mash","text":"<p>Fast genome and metagenome distance estimation using MinHash.</p> <p>Versions: 2.3</p> <p>Arches: generic</p> <p>Modules: <code>mash/2.3</code></p>"},{"location":"software/list/#masurca","title":"masurca","text":"<p>MaSuRCA is whole genome assembly software. It combines the efficiency of the de Bruijn graph and Overlap-Layout-Consensus (OLC) approaches.</p> <p>Versions: 4.0.9</p> <p>Arches: generic</p> <p>Modules: <code>masurca/4.0.9</code></p>"},{"location":"software/list/#mcl","title":"mcl","text":"<p>The MCL algorithm is short for the Markov Cluster Algorithm, a fast and scalable unsupervised cluster algorithm for graphs (also known as networks) based on simulation of (stochastic) flow in graphs.</p> <p>Versions: 14-137</p> <p>Arches: generic</p> <p>Modules: <code>mcl/14-137</code></p>"},{"location":"software/list/#megahit","title":"megahit","text":"<p>MEGAHIT: An ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph</p> <p>Versions: 1.1.4</p> <p>Arches: generic</p> <p>Modules: <code>megahit/1.1.4</code></p>"},{"location":"software/list/#meme","title":"meme","text":"<p>The MEME Suite allows the biologist to discover novel motifs in collections of unaligned nucleotide or protein sequences, and to perform a wide variety of other motif-based analyses.</p> <p>Versions: 5.3.0</p> <p>Arches: generic</p> <p>Modules: <code>meme/5.3.0</code></p>"},{"location":"software/list/#metaeuk","title":"metaeuk","text":"<p>MetaEuk is a modular toolkit designed for large-scale gene discovery and annotation in eukaryotic metagenomic contigs.</p> <p>Versions: 6-a5d39d9</p> <p>Arches: generic</p> <p>Modules: <code>metaeuk/6-a5d39d9</code></p>"},{"location":"software/list/#minced","title":"minced","text":"<p>MinCED is a program to find Clustered Regularly Interspaced Short Palindromic Repeats (CRISPRs) in full genomes or environmental datasets such as metagenomes, in which sequence size can be anywhere from 100 to 800 bp.</p> <p>Versions: 0.3.2</p> <p>Arches: generic</p> <p>Modules: <code>minced/0.3.2</code></p>"},{"location":"software/list/#miniasm","title":"miniasm","text":"<p>Miniasm is a very fast OLC-based de novo assembler for noisy long reads.</p> <p>Versions: 2018-3-30</p> <p>Arches: generic</p> <p>Modules: <code>miniasm/2018-3-30</code></p>"},{"location":"software/list/#minimap2","title":"minimap2","text":"<p>Minimap2 is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database. Mappy provides a convenient interface to minimap2.</p> <p>Versions: 2.14</p> <p>Arches: generic</p> <p>Modules: <code>minimap2/2.14</code></p>"},{"location":"software/list/#mirdeep2","title":"mirdeep2","text":"<p>miRDeep2 is a completely overhauled tool which discovers microRNA genes by analyzing sequenced RNAs.</p> <p>Versions: 0.0.8</p> <p>Arches: generic</p> <p>Modules: <code>mirdeep2/0.0.8</code></p>"},{"location":"software/list/#mmseqs2","title":"mmseqs2","text":"<p>MMseqs2 (Many-against-Many sequence searching) is a software suite to search and cluster huge protein and nucleotide sequence sets</p> <p>Versions: 14-7e284</p> <p>Arches: generic</p> <p>Modules: <code>mmseqs2/14-7e284</code></p>"},{"location":"software/list/#mothur","title":"mothur","text":"<p>This project seeks to develop a single piece of open-source, expandable software to fill the bioinformatics needs of the microbial ecology community.</p> <p>Versions: 1.48.0</p> <p>Arches: generic</p> <p>Modules: <code>mothur/1.48.0</code></p>"},{"location":"software/list/#motioncor2","title":"motioncor2","text":"<p>MotionCor2 is a multi-GPU program that corrects beam-induced sample motion recorded on dose fractionated movie stacks. It implements a robust iterative alignment algorithm that delivers precise measurement and correction of both global and non-uniform local motions at single pixel level, suitable for both single-particle and tomographic images. MotionCor2 is sufficiently fast to keep up with automated data collection.</p> <p>Versions: 1.5.0</p> <p>Arches: generic</p> <p>Modules: <code>motioncor2/1.5.0</code></p>"},{"location":"software/list/#mummer","title":"mummer","text":"<p>MUMmer is a system for rapidly aligning entire genomes.</p> <p>Versions: 3.23</p> <p>Arches: generic</p> <p>Modules: <code>mummer/3.23</code></p>"},{"location":"software/list/#mummer4","title":"mummer4","text":"<p>MUMmer is a versatil alignment tool for DNA and protein sequences.</p> <p>Versions: 4.0.0rc1</p> <p>Arches: generic</p> <p>Modules: <code>mummer4/4.0.0rc1</code></p>"},{"location":"software/list/#muscle","title":"muscle","text":"<p>MUSCLE is one of the best-performing multiple alignment programs according to published benchmark tests, with accuracy and speed that are consistently better than CLUSTALW.</p> <p>Versions: 3.8.1551</p> <p>Arches: generic</p> <p>Modules: <code>muscle/3.8.1551</code></p>"},{"location":"software/list/#ncbi-rmblastn","title":"ncbi-rmblastn","text":"<p>RMBlast search engine for NCBI</p> <p>Versions: 2.11.0</p> <p>Arches: generic</p> <p>Modules: <code>ncbi-rmblastn/2.11.0</code></p>"},{"location":"software/list/#ncbi-toolkit","title":"ncbi-toolkit","text":"<p>NCBI C++ Toolkit</p> <p>Versions: 26_0_1</p> <p>Arches: generic</p> <p>Modules: <code>ncbi-toolkit/26_0_1</code></p>"},{"location":"software/list/#ncbi-vdb","title":"ncbi-vdb","text":"<p>The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives. This package contains the interface to the VDB.</p> <p>Versions: 3.0.0</p> <p>Arches: generic</p> <p>Modules: <code>ncbi-vdb/3.0.0</code></p>"},{"location":"software/list/#nextflow","title":"nextflow","text":"<p>Data-driven computational pipelines.</p> <p>Versions: 22.10.1</p> <p>Arches: generic</p> <p>Modules: <code>nextflow/22.10.1</code></p>"},{"location":"software/list/#openjdk","title":"openjdk","text":"<p>The free and opensource java implementation</p> <p>Versions: 11.0.17_8, 16.0.2</p> <p>Arches: generic</p> <p>Modules: <code>openjdk/11.0.17_8</code>, <code>openjdk/16.0.2</code></p>"},{"location":"software/list/#openldap","title":"openldap","text":"<p>OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The suite includes: slapd - stand-alone LDAP daemon (server) libraries implementing the LDAP protocol, and utilities, tools, and sample clients.</p> <p>Versions: 2.4.49</p> <p>Arches: generic</p> <p>Modules: <code>openldap/2.4.49</code></p>"},{"location":"software/list/#openmpi","title":"openmpi","text":"<p>An open source Message Passing Interface implementation. The Open MPI Project is an open source Message Passing Interface implementation that is developed and maintained by a consortium of academic, research, and industry partners. Open MPI is therefore able to combine the expertise, technologies, and resources from all across the High Performance Computing community in order to build the best MPI library available. Open MPI offers advantages for system and software vendors, application developers and computer science researchers.</p> <p>Versions: 4.1.4</p> <p>Arches: generic</p> <p>Modules: <code>openmpi/4.1.4</code></p>"},{"location":"software/list/#orthofinder","title":"orthofinder","text":"<p>OrthoFinder is a fast, accurate and comprehensive analysis tool for comparative genomics. It finds orthologues and orthogroups infers rooted gene trees for all orthogroups and infers a rooted species tree for the species being analysed. OrthoFinder also provides comprehensive statistics for comparative genomic analyses. OrthoFinder is simple to use and all you need to run it is a set of protein sequence files (one per species) in FASTA format.</p> <p>Versions: 2.5.4</p> <p>Arches: generic</p> <p>Modules: <code>orthofinder/2.5.4</code></p>"},{"location":"software/list/#orthomcl","title":"orthomcl","text":"<p>OrthoMCL is a genome-scale algorithm for grouping orthologous protein sequences.</p> <p>Versions: 2.0.9</p> <p>Arches: generic</p> <p>Modules: <code>orthomcl/2.0.9</code></p>"},{"location":"software/list/#parallel","title":"parallel","text":"<p>GNU parallel is a shell tool for executing jobs in parallel using one or more computers. A job can be a single command or a small script that has to be run for each of the lines in the input.</p> <p>Versions: 20220522</p> <p>Arches: generic</p> <p>Modules: <code>parallel/20220522</code></p>"},{"location":"software/list/#patchelf","title":"patchelf","text":"<p>PatchELF is a small utility to modify the dynamic linker and RPATH of ELF executables.</p> <p>Versions: 0.16.1</p> <p>Arches: generic</p> <p>Modules: <code>patchelf/0.16.1</code></p>"},{"location":"software/list/#phylip","title":"phylip","text":"<p>PHYLIP (the PHYLogeny Inference Package) is a package of programs for inferring phylogenies (evolutionary trees).</p> <p>Versions: 3.697</p> <p>Arches: generic</p> <p>Modules: <code>phylip/3.697</code></p>"},{"location":"software/list/#picard","title":"picard","text":"<p>Picard is a set of command line tools for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.</p> <p>Versions: 2.26.2</p> <p>Arches: generic</p> <p>Modules: <code>picard/2.26.2</code></p>"},{"location":"software/list/#pilon","title":"pilon","text":"<p>Pilon is an automated genome assembly improvement and variant detection tool.</p> <p>Versions: 1.22</p> <p>Arches: generic</p> <p>Modules: <code>pilon/1.22</code></p>"},{"location":"software/list/#plink","title":"plink","text":"<p>PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.</p> <p>Versions: 1.07</p> <p>Arches: generic</p> <p>Modules: <code>plink/1.07</code></p>"},{"location":"software/list/#pmix","title":"pmix","text":"<p>The Process Management Interface (PMI) has been used for quite some time as a means of exchanging wireup information needed for interprocess communication. However, meeting the significant orchestration challenges presented by exascale systems requires that the process-to-system interface evolve to permit a tighter integration between the different components of the parallel application and existing and future SMS solutions. PMI Exascale (PMIx) addresses these needs by providing an extended version of the PMI definitions specifically designed to support exascale and beyond environments by: (a) adding flexibility to the functionality expressed in the existing APIs, (b) augmenting the interfaces with new APIs that provide extended capabilities, (c) forging a collaboration between subsystem providers including resource manager, fabric, file system, and programming library developers, (d) establishing a standards-like body for maintaining the definitions, and (e) providing a reference implementation of the PMIx standard that demonstrates the desired level of scalability while maintaining strict separation between it and the standard itself.</p> <p>Versions: 4.1.2</p> <p>Arches: generic</p> <p>Modules: <code>pmix/4.1.2</code></p>"},{"location":"software/list/#prokka","title":"prokka","text":"<p>Prokka is a software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.</p> <p>Versions: 1.14.6</p> <p>Arches: generic</p> <p>Modules: <code>prokka/1.14.6</code></p>"},{"location":"software/list/#r_1","title":"r","text":"<p>R is 'GNU S', a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques: linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering, etc. Please consult the R project homepage for further information.</p> <p>Versions: 4.2.0</p> <p>Arches: generic</p> <p>Modules: <code>r/4.2.0</code></p>"},{"location":"software/list/#raxml-ng","title":"raxml-ng","text":"<p>RAxML-NG is a phylogenetic tree inference tool which uses maximum- likelihood (ML) optimality criterion. Its search heuristic is based on iteratively performing a series of Subtree Pruning and Regrafting (SPR) moves, which allows to quickly navigate to the best-known ML tree. RAxML-NG is a successor of RAxML (Stamatakis 2014) and leverages the highly optimized likelihood computation implemented in libpll (Flouri et al. 2014).</p> <p>Versions: 1.0.2</p> <p>Arches: generic</p> <p>Modules: <code>raxml-ng/1.0.2</code></p>"},{"location":"software/list/#ray","title":"ray","text":"<p>Parallel genome assemblies for parallel DNA sequencing</p> <p>Versions: 2.3.1</p> <p>Arches: generic</p> <p>Modules: <code>ray/2.3.1</code></p>"},{"location":"software/list/#rclone","title":"rclone","text":"<p>Rclone is a command line program to sync files and directories to and from various cloud storage providers</p> <p>Versions: 1.59.1</p> <p>Arches: generic</p> <p>Modules: <code>rclone/1.59.1</code></p>"},{"location":"software/list/#recon","title":"recon","text":"<p>RECON: a package for automated de novo identification of repeat families from genomic sequences.</p> <p>Versions: 1.05</p> <p>Arches: generic</p> <p>Modules: <code>recon/1.05</code></p>"},{"location":"software/list/#relion","title":"relion","text":"<p>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a stand-alone computer program that employs an empirical Bayesian approach to refinement of (multiple) 3D reconstructions or 2D class averages in electron cryo-microscopy (cryo-EM).</p> <p>Versions: 4.0.1, 3.1.3, 4.0.0</p> <p>Variants: cpu, gpu</p> <p>Arches: amd, intel, intel+gpu_delay</p> <p>Modules: <code>relion/cpu/3.1.3+amd</code>, <code>relion/gpu/3.1.3+amd</code>, <code>relion/gpu/3.1.3+intel</code>, <code>relion/cpu/4.0.0+amd</code>, <code>relion/gpu/4.0.0+amd</code>, <code>relion/gpu/4.0.0+intel</code>, <code>relion/3.1.3</code>, <code>relion/4.0.0</code>, <code>relion/cpu/4.0.1+amd</code>, <code>relion/gpu/4.0.1+amd</code>, <code>relion/4.0.1</code>, <code>relion/gpu/4.0.1+intel+gpu_delay</code>, <code>relion/gpu/4.0.1+intel</code></p>"},{"location":"software/list/#relion-bbr","title":"relion-bbr","text":"<p>A modified version of Relion supporting block-based-reconstruction as described in 10.1038/s41467-018-04051-9.</p> <p>Versions: 3.1.2</p> <p>Variants: gpu</p> <p>Arches: intel</p> <p>Modules: <code>relion-bbr/gpu/3.1.2+intel</code></p>"},{"location":"software/list/#relion-helper","title":"relion-helper","text":"<p>Utilities for Relion Cryo-EM data processing on clusters.</p> <p>Versions: 0.2, 0.1, 0.3</p> <p>Arches: generic</p> <p>Modules: <code>relion-helper/0.1</code>, <code>relion-helper/0.2</code>, <code>relion-helper/0.3</code></p>"},{"location":"software/list/#repeatmasker","title":"repeatmasker","text":"<p>RepeatMasker is a program that screens DNA sequences for interspersed repeats and low complexity DNA sequences.</p> <p>Versions: 4.0.9</p> <p>Arches: generic</p> <p>Modules: <code>repeatmasker/4.0.9</code></p>"},{"location":"software/list/#repeatmodeler","title":"repeatmodeler","text":"<p>RepeatModeler is a de-novo repeat family identification and modeling package.</p> <p>Versions: 1.0.11</p> <p>Arches: generic</p> <p>Modules: <code>repeatmodeler/1.0.11</code></p>"},{"location":"software/list/#repeatscout","title":"repeatscout","text":"<p>RepeatScout - De Novo Repeat Finder, Price A.L., Jones N.C. and Pevzner P.A.</p> <p>Versions: 1.0.5</p> <p>Arches: generic</p> <p>Modules: <code>repeatscout/1.0.5</code></p>"},{"location":"software/list/#rnaquast","title":"rnaquast","text":"<p>Quality assessment of de novo transcriptome assemblies from RNA-Seq data rnaQUAST is a tool for evaluating RNA-Seq assemblies using reference genome and gene database. In addition, rnaQUAST is also capable of estimating gene database coverage by raw reads and de novo quality assessment using third-party software.</p> <p>Versions: 2.2.0</p> <p>Arches: generic</p> <p>Modules: <code>rnaquast/2.2.0</code></p>"},{"location":"software/list/#rsem","title":"rsem","text":"<p>RSEM is a software package for estimating gene and isoform expression levels from RNA-Seq data.</p> <p>Versions: 1.3.1</p> <p>Arches: generic</p> <p>Modules: <code>rsem/1.3.1</code></p>"},{"location":"software/list/#rstudio-server","title":"rstudio-server","text":"<p>RStudio is an integrated development environment (IDE) for R.</p> <p>Versions: 2022.12.0-353</p> <p>Arches: generic</p> <p>Modules: <code>rstudio-server/2022.12.0-353</code></p>"},{"location":"software/list/#sabre","title":"sabre","text":"<p>Sabre is a tool that will demultiplex barcoded reads into separate files. It will work on both single-end and paired-end data in fastq format. It simply compares the provided barcodes with each read and separates the read into its appropriate barcode file, after stripping the barcode from the read (and also stripping the quality values of the barcode bases). If a read does not have a recognized barcode, then it is put into the unknown file.</p> <p>Versions: 2013-09-27</p> <p>Arches: generic</p> <p>Modules: <code>sabre/2013-09-27</code></p>"},{"location":"software/list/#satsuma2","title":"satsuma2","text":"<p>Satsuma2 is an optimsed version of Satsuma, a tool to reliably align large and complex DNA sequences providing maximum sensitivity (to find all there is to find), specificity (to only find real homology) and speed (to accomodate the billions of base pairs in vertebrate genomes).</p> <p>Versions: 2021-03-04</p> <p>Arches: generic</p> <p>Modules: <code>satsuma2/2021-03-04</code></p>"},{"location":"software/list/#scallop","title":"scallop","text":"<p>Scallop is a reference-based transcriptome assembler for RNA-seq</p> <p>Versions: 0.10.5</p> <p>Arches: generic</p> <p>Modules: <code>scallop/0.10.5</code></p>"},{"location":"software/list/#seqprep","title":"seqprep","text":"<p>SeqPrep is a program to merge paired end Illumina reads that are overlapping into a single longer read.</p> <p>Versions: 1.3.2</p> <p>Arches: generic</p> <p>Modules: <code>seqprep/1.3.2</code></p>"},{"location":"software/list/#seqtk","title":"seqtk","text":"<p>Toolkit for processing sequences in FASTA/Q formats.</p> <p>Versions: 1.3</p> <p>Arches: generic</p> <p>Modules: <code>seqtk/1.3</code></p>"},{"location":"software/list/#sickle","title":"sickle","text":"<p>Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads.</p> <p>Versions: 1.33</p> <p>Arches: generic</p> <p>Modules: <code>sickle/1.33</code></p>"},{"location":"software/list/#slurm","title":"slurm","text":"<p>Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained. As a cluster workload manager, Slurm has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.</p> <p>Versions: 22-05-6-1</p> <p>Arches: generic</p> <p>Modules: <code>slurm/22-05-6-1</code></p>"},{"location":"software/list/#smartdenovo","title":"smartdenovo","text":"<p>SMARTdenovo is a de novo assembler for PacBio and Oxford Nanopore (ONT) data.</p> <p>Versions: master</p> <p>Arches: generic</p> <p>Modules: <code>smartdenovo/master</code></p>"},{"location":"software/list/#sortmerna","title":"sortmerna","text":"<p>SortMeRNA is a program tool for filtering, mapping and OTU-picking NGS reads in metatranscriptomic and metagenomic data</p> <p>Versions: 2017-07-13</p> <p>Arches: generic</p> <p>Modules: <code>sortmerna/2017-07-13</code></p>"},{"location":"software/list/#trimmomatic","title":"trimmomatic","text":"<p>A flexible read trimming tool for Illumina NGS data.</p> <p>Versions: 0.39</p> <p>Arches: generic</p> <p>Modules: <code>trimmomatic/0.39</code></p>"},{"location":"software/list/#ucx","title":"ucx","text":"<p>a communication library implementing high-performance messaging for MPI/PGAS frameworks</p> <p>Versions: 1.13.1</p> <p>Arches: generic</p> <p>Modules: <code>ucx/1.13.1</code></p>"},{"location":"software/modules/","title":"Module System","text":""},{"location":"software/modules/#intro","title":"Intro","text":"<p>High performance compute clusters usually have a variety of software with sometimes conflicting dependencies. Software packages may need to make modifications to the user environment, or the same software may be compiled multiple times to run efficiently on differing hardware within the cluster. To support these use cases, software is managed with a module system that prepares the user environment to access specific software on load and returns the environment to its former state when unloaded. A module is the bit of code that enacts and tracks these changes to the user environment, and the module system is software that runs these modules and the collection of modules it is aware of. Most often, a module is associated with a specific software package at a specific version, but they can also be used to make more general changes to a user environment; for example, a module could load a set of configurations for the BASH shell that set color themes.</p> <p>The two most commonly deployed module systems are environment modules (or <code>envmod</code>) and lmod. Franklin currently uses <code>lmod</code>, which is cross-compatible with <code>envmod</code>.</p>"},{"location":"software/modules/#usage","title":"Usage","text":"<p>The <code>module</code> command is the entry point for users to manage modules in their environment. All module operations will be of the form <code>module [SUBCOMMAND]</code>. Usage information is available on the cluster by running <code>module --help</code>.</p> <p>The basic commands are: <code>module load [MODULENAME]</code> to load a module into your environment; <code>module unload [MODULENAME]</code> to remove that module; <code>module avail</code> to see modules available for loading; and <code>module list</code> to see which modules are currently loaded. We will go over these commands, and some additional commands, in the following sections.</p>"},{"location":"software/modules/#listing","title":"Listing","text":""},{"location":"software/modules/#module-avail","title":"<code>module avail</code>","text":"<p>Lists the modules currently available to load on the system. Some example output would be:</p> <pre><code>$ module avail\n\n----------------------- /share/apps/franklin/modulefiles ------------------------\n   conda/base/4.X                conda/cryolo/1.8.4-cuda-11 (D)    testmod/1.0\n   conda/cryolo/1.8.4-cuda-10    conda/rockstar/0.1\n\n---------------------- /share/apps/spack/modulefiles/Core -----------------------\n...\n   gatk/3.8.1                         pmix/4.1.2             (L)\n   gatk/4.2.6.1              (D)      prokka/1.14.6\n   gcc/4.9.4                 (L)      raxml-ng/1.0.2\n   gcc/5.5.0                          ray/2.3.1\n   gcc/7.5.0                 (D)      recon/1.05\n   gctf/1.06                 (L)      relion-helper/0.1\n   genrich/0.6                        relion-helper/0.2      (L,D)\n...\n</code></pre> <p>Each entry corresponds to software available for load. Different sections will appear depending on loaded prerequisites, which you can read about under <code>module spider</code>. Where there are multiple versions or variants of a module, a <code>(D)</code> will be listed next to the name of the default version. An <code>(L)</code> indicates that module is currently loaded.</p> <p>Note that this does not necessarily list every possible module on the system. Some software is compiled with a specific compiler and compiler version, and the relevant compiler module must be loaded first. To see all possible modules, use the <code>module spider</code> command.</p>"},{"location":"software/modules/#module-spider","title":"<code>module spider</code>","text":"<p>Lists all possible modules that could be loaded. Some modules require a specific compiler version to be loaded, and these modules will not be listed in <code>module avail</code> unless that module is loaded. Module spider will list these modules anyway. If you run the command with a specific module, it will list the prerequisite modules required to make said module available. For example, if we try to run:</p> <pre><code>$ module load megahit\n\nLmod has detected the following error:  These module(s) or extension(s) exist but cannot be loaded as requested: \"megahit\"\n   Try: \"module spider megahit\" to see how to load the module(s).\n</code></pre> <p>...the load fails. If we then use <code>spider</code>:</p> <pre><code>$ module spider megahit\n\n-----------------------------------------------------------------------------\n  megahit: megahit/1.1.4\n-----------------------------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before\n    the \"megahit/1.1.4\" module is available to load.\n\n      gcc/4.9.4\n\n    Help:\n      MEGAHIT: An ultra-fast single-node solution for large and complex\n      metagenomics assembly via succinct de Bruijn graph\n</code></pre> <p>...we see that we have to first load <code>gcc/4.9.4</code>. Let's do that:</p> <pre><code>$ module load gcc/4.9.4\ngcc/4.9.4: loaded.\n\n$ module avail\n\n-------------------- /share/apps/spack/modulefiles/gcc/4.9.4 --------------------\n   megahit/1.1.4\n\n----------------------- /share/apps/franklin/modulefiles ------------------------\n   conda/base/4.X                conda/cryolo/1.8.4-cuda-11 (D)\n   conda/cryolo/1.8.4-cuda-10    conda/rockstar/0.1\n\n---------------------- /share/apps/spack/modulefiles/Core -----------------------\n   StdEnv                    (L)    libevent/2.1.12        (L)\n   abyss/2.3.1                      mash/2.3\n</code></pre> <p>We are now presented with a new section for those modules that require <code>gcc/4.9.4</code>, with the <code>megahit</code> module listed there. Now, we can load it:</p> <pre><code>$ module load megahit\n\nmegahit/1.1.4: loaded.\n</code></pre> <p>Running <code>module spider</code> without any arguments will list all the modules that could be loaded, unlike <code>module avail</code>, which will list only the modules available for load given any currently-loaded prerequisites.</p>"},{"location":"software/modules/#module-list","title":"<code>module list</code>","text":"<p>Lists the modules currently loaded in the user environment. By default, the output should be similar to:</p> <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) cuda/11.8.0   3) libevent/2.1.12   5) slurm/22-05-6-1   7) openmpi/4.1.4\n  2) hwloc/2.8.0   4) pmix/4.1.2        6) ucx/1.13.1\n</code></pre> <p>Additional modules will be added or removed as you load and unload them.</p>"},{"location":"software/modules/#module-overview","title":"<code>module overview</code>","text":"<p>This will give a condensed overview of available modules. It is most useful when the list of modules is very large, and there are multiple versions of each module available. The output is a list of each module name followed by the number of versions of the module.</p> <pre><code>$ module overview\n\n----------------------- /share/apps/franklin/modulefiles ------------------------\nconda/base (1)   conda/cryolo (2)   conda/rockstar (1)   testmod (1)\n\n---------------------- /share/apps/spack/modulefiles/Core -----------------------\nStdEnv         (1)   hmmer            (1)   ncbi-toolkit  (1)\nabyss          (1)   homer            (1)   ncbi-vdb      (1)\namdfftw        (1)   hwloc            (1)   nextflow      (1)\naragorn        (1)   igv              (1)   openmpi       (1)\nbedtools2      (1)   infernal         (1)   orthofinder   (1)\nblast-plus     (1)   intel-oneapi-mkl (1)   orthomcl      (1)\nblast2go       (1)   interproscan     (1)   parallel      (1)\nblat           (1)   iq-tree          (1)   patchelf      (1)\n</code></pre>"},{"location":"software/modules/#loading-and-unloading","title":"Loading and Unloading","text":""},{"location":"software/modules/#module-load","title":"<code>module load</code>","text":"<p>This loads the requested module into the active environment. Loading a module can edit environment variables, such as prepending directories to <code>$PATH</code> so that the executables within can be run, set and unset new or existing environment variables, define shell functions, and generally, modify your user environment arbitrarily. The modifications it makes are tracked, so that when the module is eventually unloaded, any changes can be returned to their former state.</p> <p>Let's load a module.</p> <pre><code>$ module load bwa/0.7.17\nbwa/0.7.17: loaded.\n</code></pre> <p>Now, you have access to the <code>bwa</code> executable. If you try to run <code>bwa mem</code>, you'll get its help output. This also sets the appopriate variables so that you can now run <code>man bwa</code> to view its manpage.</p> <p>Note that some modules have multiple versions. Running <code>module load [MODULENAME]</code> without specifying a version will load the latest version, unless a default has been specified. When there are multiple versions, a <code>(D)</code> will be printed next to the default version when using <code>module avail</code>.</p> <p>Some modules are nested under a deeper hierarchy. For example, <code>relion</code> has six variants, two under <code>relion/cpu</code> and four under <code>relion/gpu</code>. To load these, you must specify the second layer of hierarchy: <code>module load relion</code> will fail, but <code>module load relion/cpu</code> will load the default module under <code>relion/cpu</code>, which has the full name <code>relion/cpu/4.0.0+amd</code>. More information on this system can be found under Organization.</p> <p>The modules on Franklin are all configured to set a <code>$NAME_ROOT</code> variable that points to the installation prefix. This will correspond to the name of the module, minus the version. For example:</p> <pre><code>$  ls -R $BWA_ROOT\n/share/apps/spack/spack-v0.19/opt/spack/linux-ubuntu22.04-x86_64_v3/gcc-9.5.0/bwa-0.7.17-3ogkbh2ixha52dxps2letankhc2dbeax:\nbin  doc  man\n\n...\n</code></pre> <p>Usually, this will be a very long pathname, as most software on the cluster is managed via the spack build system. This would be most useful if you're developing software on the cluster.</p>"},{"location":"software/modules/#organization","title":"Organization","text":"<p>Many modules correspond to different versions of the same software, and some software has multiple variants of the same version. The default naming convention is <code>NAME/VERSION</code>: for example, <code>cuda/11.8.0</code> or <code>mcl/14-137</code>. The version can be omitted when loading, in which case the highest-versioned module or the version marked as default (with a <code>(D)</code>) will be used.</p>"},{"location":"software/modules/#variants","title":"Variants","text":"<p>Some module names are structured as <code>NAME/VARIANT/VERSION</code>. For these, the minimum name you can use for loading is <code>NAME/VARIANT</code>: for example, you can load <code>relion/gpu</code> or <code>relion/cpu</code>, but just trying to <code>module load relion</code> will fail.</p>"},{"location":"software/modules/#architectures","title":"Architectures","text":"<p>Software is sometimes compiled with optimizations specific to certain hardware. These are named with the format <code>NAME/VERSION+ARCH</code> or <code>NAME/VARIANT/VERSION+arch</code>. For example, <code>ctffind/4.1.14+amd</code> was compiled with AMD Zen2-specific optimizations and uses the <code>amdfftw</code> implementation of the <code>FFTW</code> library, and will fail on the Intel-based RTX2080 nodes purchased by the Al-Bassam lab (<code>gpu-9-[10,18,26]</code>). Conversely, <code>ctffind/4.1.14+intel</code> was compiled with Intel-specific compiler optimizations as well as linking against the Intel OneAPI MKL implementation of <code>FFTW</code>, and is only meant to be used on those nodes. In all cases, the <code>+amd</code> variant of a module, if it exists, is the default, as the majority of the nodes use AMD CPUs.</p> <p>Software without a <code>+ARCH</code> was compiled for a generic architecture and will function on all nodes. The generic architecture on Franklin is <code>x86-64-v3</code>, which means they support <code>AVX</code>, <code>AVX2</code>, and all other previous <code>SSE</code> and other vectorized instructions.</p>"},{"location":"software/modules/#conda-environments","title":"Conda Environments","text":"<p>The various conda modules have their own naming scheme. These are of the form <code>conda/ENVIRONMENT/VERSION</code>. The <code>conda/base/VERSION</code> module(s) load the base conda environment and set the appropriate variables to use the <code>conda activate</code> and <code>deactivate</code> commands, while the the modules for the other environments first load <code>conda/base</code> and then activate the environment to which they correspond. The the <code>conda</code> section for more information on <code>conda</code> and Python on Franklin.</p>"},{"location":"software/rlang/","title":":simple-rstudio: R and RStudio","text":""}]}